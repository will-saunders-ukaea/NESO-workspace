====
Starting job 6207187 at Thu  4 May 16:59:50 BST 2023 for user dc-saun1.
Running on nodes: b[103-110]
====
Using hipSYCL OpenMP host device
Kernel type: GPU
setup step: 0
setup step: 1
setup step: 2
setup step: 3
setup step: 4
setup step: 5
setup step: 6
setup step: 7
setup step: 8
setup step: 9
setup step: 10
setup step: 11
setup step: 12
setup step: 13
setup step: 14
setup step: 15
setup step: 16
setup step: 17
setup step: 18
setup step: 19
setup step: 20
setup step: 21
setup step: 22
setup step: 23
setup step: 24
setup step: 25
setup step: 26
setup step: 27
setup step: 28
setup step: 29
setup step: 30
setup step: 31
setup step: 32
setup step: 33
setup step: 34
setup step: 35
setup step: 36
setup step: 37
setup step: 38
setup step: 39
setup step: 40
setup step: 41
setup step: 42
setup step: 43
setup step: 44
setup step: 45
setup step: 46
setup step: 47
setup step: 48
setup step: 49
setup step: 50
setup step: 51
setup step: 52
setup step: 53
setup step: 54
setup step: 55
setup step: 56
setup step: 57
setup step: 58
setup step: 59
setup step: 60
setup step: 61
setup step: 62
setup step: 63
setup step: 64
setup step: 65
setup step: 66
setup step: 67
setup step: 68
setup step: 69
setup step: 70
setup step: 71
setup step: 72
setup step: 73
setup step: 74
setup step: 75
setup step: 76
setup step: 77
setup step: 78
setup step: 79
setup step: 80
setup step: 81
setup step: 82
setup step: 83
setup step: 84
setup step: 85
setup step: 86
setup step: 87
setup step: 88
setup step: 89
setup step: 90
setup step: 91
setup step: 92
setup step: 93
setup step: 94
setup step: 95
setup step: 96
setup step: 97
setup step: 98
setup step: 99
setup step: 100
setup step: 101
setup step: 102
setup step: 103
setup step: 104
setup step: 105
setup step: 106
setup step: 107
setup step: 108
setup step: 109
setup step: 110
setup step: 111
setup step: 112
setup step: 113
setup step: 114
setup step: 115
setup step: 116
setup step: 117
setup step: 118
setup step: 119
setup step: 120
setup step: 121
setup step: 122
setup step: 123
setup step: 124
setup step: 125
setup step: 126
setup step: 127
setup step: 128
setup step: 129
setup step: 130
setup step: 131
setup step: 132
setup step: 133
setup step: 134
setup step: 135
setup step: 136
setup step: 137
setup step: 138
setup step: 139
setup step: 140
setup step: 141
setup step: 142
setup step: 143
setup step: 144
setup step: 145
setup step: 146
setup step: 147
setup step: 148
setup step: 149
setup step: 150
setup step: 151
setup step: 152
setup step: 153
setup step: 154
setup step: 155
setup step: 156
setup step: 157
setup step: 158
setup step: 159
setup step: 160
setup step: 161
setup step: 162
setup step: 163
setup step: 164
setup step: 165
setup step: 166
setup step: 167
setup step: 168
setup step: 169
setup step: 170
setup step: 171
setup step: 172
setup step: 173
setup step: 174
setup step: 175
setup step: 176
setup step: 177
setup step: 178
setup step: 179
setup step: 180
setup step: 181
setup step: 182
setup step: 183
setup step: 184
setup step: 185
setup step: 186
setup step: 187
setup step: 188
setup step: 189
setup step: 190
setup step: 191
setup step: 192
setup step: 193
setup step: 194
setup step: 195
setup step: 196
setup step: 197
setup step: 198
setup step: 199
setup step: 200
setup step: 201
setup step: 202
setup step: 203
setup step: 204
setup step: 205
setup step: 206
setup step: 207
setup step: 208
setup step: 209
setup step: 210
setup step: 211
setup step: 212
setup step: 213
setup step: 214
setup step: 215
setup step: 216
setup step: 217
setup step: 218
setup step: 219
setup step: 220
setup step: 221
setup step: 222
setup step: 223
setup step: 224
setup step: 225
setup step: 226
setup step: 227
setup step: 228
setup step: 229
setup step: 230
setup step: 231
setup step: 232
setup step: 233
setup step: 234
setup step: 235
setup step: 236
setup step: 237
setup step: 238
setup step: 239
setup step: 240
setup step: 241
setup step: 242
setup step: 243
setup step: 244
setup step: 245
setup step: 246
setup step: 247
setup step: 248
setup step: 249
setup step: 250
setup step: 251
setup step: 252
setup step: 253
setup step: 254
setup step: 255
setup step: 256
setup step: 257
setup step: 258
setup step: 259
setup step: 260
setup step: 261
setup step: 262
setup step: 263
setup step: 264
setup step: 265
setup step: 266
setup step: 267
setup step: 268
setup step: 269
setup step: 270
setup step: 271
setup step: 272
setup step: 273
setup step: 274
setup step: 275
setup step: 276
setup step: 277
setup step: 278
setup step: 279
setup step: 280
setup step: 281
setup step: 282
setup step: 283
setup step: 284
setup step: 285
setup step: 286
setup step: 287
setup step: 288
setup step: 289
setup step: 290
setup step: 291
setup step: 292
setup step: 293
setup step: 294
setup step: 295
setup step: 296
setup step: 297
setup step: 298
setup step: 299
setup step: 300
setup step: 301
setup step: 302
setup step: 303
setup step: 304
setup step: 305
setup step: 306
setup step: 307
setup step: 308
setup step: 309
setup step: 310
setup step: 311
setup step: 312
setup step: 313
setup step: 314
setup step: 315
setup step: 316
setup step: 317
setup step: 318
setup step: 319
setup step: 320
setup step: 321
setup step: 322
setup step: 323
setup step: 324
setup step: 325
setup step: 326
setup step: 327
setup step: 328
setup step: 329
setup step: 330
setup step: 331
setup step: 332
setup step: 333
setup step: 334
setup step: 335
setup step: 336
setup step: 337
setup step: 338
setup step: 339
setup step: 340
setup step: 341
setup step: 342
setup step: 343
setup step: 344
setup step: 345
setup step: 346
setup step: 347
setup step: 348
setup step: 349
setup step: 350
setup step: 351
setup step: 352
setup step: 353
setup step: 354
setup step: 355
setup step: 356
setup step: 357
setup step: 358
setup step: 359
setup step: 360
setup step: 361
setup step: 362
setup step: 363
setup step: 364
setup step: 365
setup step: 366
setup step: 367
setup step: 368
setup step: 369
setup step: 370
setup step: 371
setup step: 372
setup step: 373
setup step: 374
setup step: 375
setup step: 376
setup step: 377
setup step: 378
setup step: 379
setup step: 380
setup step: 381
setup step: 382
setup step: 383
setup step: 384
setup step: 385
setup step: 386
setup step: 387
setup step: 388
setup step: 389
setup step: 390
setup step: 391
setup step: 392
setup step: 393
setup step: 394
setup step: 395
setup step: 396
setup step: 397
setup step: 398
setup step: 399
Particle count  : 3200000
Particle Weight : 3.28987e-05
step: 0 0.0282061
step: 10 0.0232472
step: 20 0.0246675
step: 30 0.0244786
step: 40 0.0245159
step: 50 0.0246606
step: 60 0.0246741
step: 70 0.0247716
step: 80 0.024768
step: 90 0.0247019
step: 100 0.0246754
step: 110 0.0245407
step: 120 0.0245133
step: 130 0.0243562
step: 140 0.0242701
step: 150 0.0241726
step: 160 0.0240567
step: 170 0.0239791
step: 180 0.0238643
step: 190 0.0237938
Time taken: 4.745
Time taken per step: 0.023725
BENCHMARK Time taken: 2.27576
BENCHMARK Time taken per step: 0.0227576
Elapsed Time: 21.531s
    CPU Time: 346.167s
        Effective Time: 245.671s
        Spin Time: 100.365s
         | A significant portion of CPU time is spent waiting. Use this metric
         | to discover which synchronizations are spinning. Consider adjusting
         | spin wait parameters, changing the lock implementation (for example,
         | by backing off then descheduling), or adjusting the synchronization
         | granularity.
         |
            Imbalance or Serial Spinning: 95.937s
             | The threading runtime function related to time spent on imbalance
             | or serial spinning consumed a significant amount of CPU time.
             | This can be caused by a load imbalance, insufficient concurrency
             | for all working threads, or busy waits of worker threads while
             | serial code is executed. If there is an imbalance, apply dynamic
             | work scheduling or reduce the size of work chunks or tasks. If
             | there is insufficient concurrency, consider collapsing the outer
             | and inner loops. If there is a wait for completion of serial
             | code, explore options for parallelization with Intel Advisor,
             | algorithm, or microarchitecture tuning of the application's
             | serial code with VTune Profiler Basic Hotspots or
             | Microarchitecture Exploration analysis respectively. For OpenMP*
             | applications, use the Per-Barrier OpenMP Potential Gain metric
             | set in the HPC Performance Characterization analysis to discover
             | the reason for high imbalance or serial spin time.
             |
            Lock Contention: 0s
            Other: 4.428s
        Overhead Time: 0.130s
            Creation: 0.130s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 144
    Paused Time: 0s

Top Hotspots
Function                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Module             CPU Time  % of CPU Time(%)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------  --------  ----------------
gomp_simple_barrier_wait                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              libgomp.so.1        81.898s             23.7%
MPI_Bcast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             libmpi.so.40        22.779s              6.6%
MPI_Wait                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              libmpi.so.40        19.484s              5.6%
_ZN7hipsycl4glue12omp_dispatch29reducible_parallel_invocationIZNS1_19parallel_for_kernelILi1EZZN4NESO16BaryEvaluateBaseIN6Nektar12MultiRegions9ContFieldEE8evaluateIdNS6_5ArrayINS6_4OneDEdEEEEvSt10shared_ptrINS4_9Particles13ParticleGroupEENSF_3SymIT_EEiRKT0_ENKUlRNS_4sycl7handlerEE_clESQ_EUlNSO_2idILi1EEEE_JEEEvSL_NSO_5rangeIXT_EEEDpT1_EUlDpRT_E_JEEEvSJ_DpT0_._omp_fn.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Electrostatic2D3V   15.241s              4.4%
std::_Function_handler<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>), hipsycl::glue::omp_dispatch::parallel_for_ndrange_kernel<(int)2, NESO::NektarGraphLocalMapperT::map_native_2d(NESO::Particles::ParticleGroup&, int)::{lambda(hipsycl::sycl::handler&)#1}::operator()(hipsycl::sycl::handler&) const::{lambda(hipsycl::sycl::nd_item<(int)2>)#1}>(void, (int)2, hipsycl::sycl::range<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)>, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>), hipsycl::sycl::id<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)>, unsigned long, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>))::{lambda(void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)&)#1}::operator()<>(, signed char, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)) const::{lambda(hipsycl::sycl::id<(int)2>hipsycl::sycl::id<(int)2>)#2}>::_M_invoke  Electrostatic2D3V   13.423s              3.9%
[Others]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              N/A                193.343s             55.9%
Collection and Platform Info
    Application Command Line: /cosma5/data/do009/dc-saun1/git/NESO3D/solvers/Electrostatic2D3V/Electrostatic2D3V "conditions.xml" "square_32x32.xml" 
    Operating System: 3.10.0-1160.83.1.el7.x86_64 \S Kernel \r on an \m 
    MPI Process Rank: 99
    Computer Name: b109.pri.cosma7.alces.network
    Result Size: 23.7 MB 
    Collection start time: 15:59:54 04/05/2023 UTC
    Collection stop time: 16:00:26 04/05/2023 UTC
    Collector Type: User-mode sampling and tracing
    CPU
        Name: Unknown
        Frequency: 2.994 GHz
        Logical CPU Count: 64
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
Elapsed Time: 21.520s
    CPU Time: 342.650s
        Effective Time: 241.545s
        Spin Time: 100.945s
         | A significant portion of CPU time is spent waiting. Use this metric
         | to discover which synchronizations are spinning. Consider adjusting
         | spin wait parameters, changing the lock implementation (for example,
         | by backing off then descheduling), or adjusting the synchronization
         | granularity.
         |
            Imbalance or Serial Spinning: 96.014s
             | The threading runtime function related to time spent on imbalance
             | or serial spinning consumed a significant amount of CPU time.
             | This can be caused by a load imbalance, insufficient concurrency
             | for all working threads, or busy waits of worker threads while
             | serial code is executed. If there is an imbalance, apply dynamic
             | work scheduling or reduce the size of work chunks or tasks. If
             | there is insufficient concurrency, consider collapsing the outer
             | and inner loops. If there is a wait for completion of serial
             | code, explore options for parallelization with Intel Advisor,
             | algorithm, or microarchitecture tuning of the application's
             | serial code with VTune Profiler Basic Hotspots or
             | Microarchitecture Exploration analysis respectively. For OpenMP*
             | applications, use the Per-Barrier OpenMP Potential Gain metric
             | set in the HPC Performance Characterization analysis to discover
             | the reason for high imbalance or serial spin time.
             |
            Lock Contention: 0s
            Other: 4.931s
        Overhead Time: 0.160s
            Creation: 0.160s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 144
    Paused Time: 0s

Top Hotspots
Function                                                                                                                                                                                                                                                                                                                                                                            Module             CPU Time  % of CPU Time(%)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------  --------  ----------------
gomp_simple_barrier_wait                                                                                                                                                                                                                                                                                                                                                            libgomp.so.1        84.116s             24.5%
MPI_Wait                                                                                                                                                                                                                                                                                                                                                                            libmpi.so.40        24.985s              7.3%
MPI_Bcast                                                                                                                                                                                                                                                                                                                                                                           libmpi.so.40        23.459s              6.8%
_ZN7hipsycl4glue12omp_dispatch29reducible_parallel_invocationIZNS1_19parallel_for_kernelILi1EZZN4NESO16BaryEvaluateBaseIN6Nektar12MultiRegions9ContFieldEE8evaluateIdNS6_5ArrayINS6_4OneDEdEEEEvSt10shared_ptrINS4_9Particles13ParticleGroupEENSF_3SymIT_EEiRKT0_ENKUlRNS_4sycl7handlerEE_clESQ_EUlNSO_2idILi1EEEE_JEEEvSL_NSO_5rangeIXT_EEEDpT1_EUlDpRT_E_JEEEvSJ_DpT0_._omp_fn.0  Electrostatic2D3V   15.091s              4.4%
uct_mm_iface_progress                                                                                                                                                                                                                                                                                                                                                               libuct.so.0         12.770s              3.7%
[Others]                                                                                                                                                                                                                                                                                                                                                                            N/A                182.228s             53.2%
Collection and Platform Info
    Application Command Line: /cosma5/data/do009/dc-saun1/git/NESO3D/solvers/Electrostatic2D3V/Electrostatic2D3V "conditions.xml" "square_32x32.xml" 
    Operating System: 3.10.0-1160.83.1.el7.x86_64 \S Kernel \r on an \m 
    MPI Process Rank: 49
    Computer Name: b106.pri.cosma7.alces.network
    Result Size: 23.3 MB 
    Collection start time: 15:59:55 04/05/2023 UTC
    Collection stop time: 16:00:26 04/05/2023 UTC
    Collector Type: User-mode sampling and tracing
    CPU
        Name: Unknown
        Frequency: 2.994 GHz
        Logical CPU Count: 64
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
Elapsed Time: 21.349s
    CPU Time: 338.350s
        Effective Time: 236.941s
        Spin Time: 101.278s
         | A significant portion of CPU time is spent waiting. Use this metric
         | to discover which synchronizations are spinning. Consider adjusting
         | spin wait parameters, changing the lock implementation (for example,
         | by backing off then descheduling), or adjusting the synchronization
         | granularity.
         |
            Imbalance or Serial Spinning: 96.859s
             | The threading runtime function related to time spent on imbalance
             | or serial spinning consumed a significant amount of CPU time.
             | This can be caused by a load imbalance, insufficient concurrency
             | for all working threads, or busy waits of worker threads while
             | serial code is executed. If there is an imbalance, apply dynamic
             | work scheduling or reduce the size of work chunks or tasks. If
             | there is insufficient concurrency, consider collapsing the outer
             | and inner loops. If there is a wait for completion of serial
             | code, explore options for parallelization with Intel Advisor,
             | algorithm, or microarchitecture tuning of the application's
             | serial code with VTune Profiler Basic Hotspots or
             | Microarchitecture Exploration analysis respectively. For OpenMP*
             | applications, use the Per-Barrier OpenMP Potential Gain metric
             | set in the HPC Performance Characterization analysis to discover
             | the reason for high imbalance or serial spin time.
             |
            Lock Contention: 0s
            Other: 4.419s
        Overhead Time: 0.130s
            Creation: 0.130s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 145
    Paused Time: 0s

Top Hotspots
Function                                                                                                                                                                                                                                                                                                                                                                            Module             CPU Time  % of CPU Time(%)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------  --------  ----------------
gomp_simple_barrier_wait                                                                                                                                                                                                                                                                                                                                                            libgomp.so.1        85.728s             25.3%
MPI_Wait                                                                                                                                                                                                                                                                                                                                                                            libmpi.so.40        25.444s              7.5%
MPI_Bcast                                                                                                                                                                                                                                                                                                                                                                           libmpi.so.40        21.078s              6.2%
_ZN7hipsycl4glue12omp_dispatch29reducible_parallel_invocationIZNS1_19parallel_for_kernelILi1EZZN4NESO16BaryEvaluateBaseIN6Nektar12MultiRegions9ContFieldEE8evaluateIdNS6_5ArrayINS6_4OneDEdEEEEvSt10shared_ptrINS4_9Particles13ParticleGroupEENSF_3SymIT_EEiRKT0_ENKUlRNS_4sycl7handlerEE_clESQ_EUlNSO_2idILi1EEEE_JEEEvSL_NSO_5rangeIXT_EEEDpT1_EUlDpRT_E_JEEEvSJ_DpT0_._omp_fn.0  Electrostatic2D3V   14.852s              4.4%
uct_mm_iface_progress                                                                                                                                                                                                                                                                                                                                                               libuct.so.0         13.830s              4.1%
[Others]                                                                                                                                                                                                                                                                                                                                                                            N/A                177.419s             52.4%
Collection and Platform Info
    Application Command Line: /cosma5/data/do009/dc-saun1/git/NESO3D/solvers/Electrostatic2D3V/Electrostatic2D3V "conditions.xml" "square_32x32.xml" 
    Operating System: 3.10.0-1160.83.1.el7.x86_64 \S Kernel \r on an \m 
    MPI Process Rank: 15
    Computer Name: b103.pri.cosma7.alces.network
    Result Size: 24.4 MB 
    Collection start time: 15:59:54 04/05/2023 UTC
    Collection stop time: 16:00:26 04/05/2023 UTC
    Collector Type: User-mode sampling and tracing
    CPU
        Name: Unknown
        Frequency: 2.994 GHz
        Logical CPU Count: 64
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
Elapsed Time: 21.394s
    CPU Time: 346.580s
        Effective Time: 245.363s
        Spin Time: 101.087s
         | A significant portion of CPU time is spent waiting. Use this metric
         | to discover which synchronizations are spinning. Consider adjusting
         | spin wait parameters, changing the lock implementation (for example,
         | by backing off then descheduling), or adjusting the synchronization
         | granularity.
         |
            Imbalance or Serial Spinning: 96.537s
             | The threading runtime function related to time spent on imbalance
             | or serial spinning consumed a significant amount of CPU time.
             | This can be caused by a load imbalance, insufficient concurrency
             | for all working threads, or busy waits of worker threads while
             | serial code is executed. If there is an imbalance, apply dynamic
             | work scheduling or reduce the size of work chunks or tasks. If
             | there is insufficient concurrency, consider collapsing the outer
             | and inner loops. If there is a wait for completion of serial
             | code, explore options for parallelization with Intel Advisor,
             | algorithm, or microarchitecture tuning of the application's
             | serial code with VTune Profiler Basic Hotspots or
             | Microarchitecture Exploration analysis respectively. For OpenMP*
             | applications, use the Per-Barrier OpenMP Potential Gain metric
             | set in the HPC Performance Characterization analysis to discover
             | the reason for high imbalance or serial spin time.
             |
            Lock Contention: 0s
            Other: 4.550s
        Overhead Time: 0.130s
            Creation: 0.130s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 144
    Paused Time: 0s

Top Hotspots
Function                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Module             CPU Time  % of CPU Time(%)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------  --------  ----------------
gomp_simple_barrier_wait                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              libgomp.so.1        83.296s             24.0%
MPI_Bcast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             libmpi.so.40        23.410s              6.8%
MPI_Wait                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              libmpi.so.40        20.417s              5.9%
_ZN7hipsycl4glue12omp_dispatch29reducible_parallel_invocationIZNS1_19parallel_for_kernelILi1EZZN4NESO16BaryEvaluateBaseIN6Nektar12MultiRegions9ContFieldEE8evaluateIdNS6_5ArrayINS6_4OneDEdEEEEvSt10shared_ptrINS4_9Particles13ParticleGroupEENSF_3SymIT_EEiRKT0_ENKUlRNS_4sycl7handlerEE_clESQ_EUlNSO_2idILi1EEEE_JEEEvSL_NSO_5rangeIXT_EEEDpT1_EUlDpRT_E_JEEEvSJ_DpT0_._omp_fn.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Electrostatic2D3V   14.819s              4.3%
std::_Function_handler<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>), hipsycl::glue::omp_dispatch::parallel_for_ndrange_kernel<(int)2, NESO::NektarGraphLocalMapperT::map_native_2d(NESO::Particles::ParticleGroup&, int)::{lambda(hipsycl::sycl::handler&)#1}::operator()(hipsycl::sycl::handler&) const::{lambda(hipsycl::sycl::nd_item<(int)2>)#1}>(void, (int)2, hipsycl::sycl::range<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)>, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>), hipsycl::sycl::id<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)>, unsigned long, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>))::{lambda(void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)&)#1}::operator()<>(, signed char, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)) const::{lambda(hipsycl::sycl::id<(int)2>hipsycl::sycl::id<(int)2>)#2}>::_M_invoke  Electrostatic2D3V   12.578s              3.6%
[Others]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              N/A                192.060s             55.4%
Collection and Platform Info
    Application Command Line: /cosma5/data/do009/dc-saun1/git/NESO3D/solvers/Electrostatic2D3V/Electrostatic2D3V "conditions.xml" "square_32x32.xml" 
    Operating System: 3.10.0-1160.83.1.el7.x86_64 \S Kernel \r on an \m 
    MPI Process Rank: 24
    Computer Name: b104.pri.cosma7.alces.network
    Result Size: 24.5 MB 
    Collection start time: 15:59:54 04/05/2023 UTC
    Collection stop time: 16:00:26 04/05/2023 UTC
    Collector Type: User-mode sampling and tracing
    CPU
        Name: Unknown
        Frequency: 2.994 GHz
        Logical CPU Count: 64
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
Elapsed Time: 21.325s
    CPU Time: 346.610s
        Effective Time: 246.002s
        Spin Time: 100.517s
         | A significant portion of CPU time is spent waiting. Use this metric
         | to discover which synchronizations are spinning. Consider adjusting
         | spin wait parameters, changing the lock implementation (for example,
         | by backing off then descheduling), or adjusting the synchronization
         | granularity.
         |
            Imbalance or Serial Spinning: 95.816s
             | The threading runtime function related to time spent on imbalance
             | or serial spinning consumed a significant amount of CPU time.
             | This can be caused by a load imbalance, insufficient concurrency
             | for all working threads, or busy waits of worker threads while
             | serial code is executed. If there is an imbalance, apply dynamic
             | work scheduling or reduce the size of work chunks or tasks. If
             | there is insufficient concurrency, consider collapsing the outer
             | and inner loops. If there is a wait for completion of serial
             | code, explore options for parallelization with Intel Advisor,
             | algorithm, or microarchitecture tuning of the application's
             | serial code with VTune Profiler Basic Hotspots or
             | Microarchitecture Exploration analysis respectively. For OpenMP*
             | applications, use the Per-Barrier OpenMP Potential Gain metric
             | set in the HPC Performance Characterization analysis to discover
             | the reason for high imbalance or serial spin time.
             |
            Lock Contention: 0s
            Other: 4.701s
        Overhead Time: 0.090s
            Creation: 0.090s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 144
    Paused Time: 0s

Top Hotspots
Function                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Module             CPU Time  % of CPU Time(%)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------  --------  ----------------
gomp_simple_barrier_wait                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              libgomp.so.1        81.967s             23.6%
MPI_Bcast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             libmpi.so.40        22.319s              6.4%
MPI_Wait                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              libmpi.so.40        21.112s              6.1%
_ZN7hipsycl4glue12omp_dispatch29reducible_parallel_invocationIZNS1_19parallel_for_kernelILi1EZZN4NESO16BaryEvaluateBaseIN6Nektar12MultiRegions9ContFieldEE8evaluateIdNS6_5ArrayINS6_4OneDEdEEEEvSt10shared_ptrINS4_9Particles13ParticleGroupEENSF_3SymIT_EEiRKT0_ENKUlRNS_4sycl7handlerEE_clESQ_EUlNSO_2idILi1EEEE_JEEEvSL_NSO_5rangeIXT_EEEDpT1_EUlDpRT_E_JEEEvSJ_DpT0_._omp_fn.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Electrostatic2D3V   14.420s              4.2%
std::_Function_handler<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>), hipsycl::glue::omp_dispatch::parallel_for_ndrange_kernel<(int)2, NESO::NektarGraphLocalMapperT::map_native_2d(NESO::Particles::ParticleGroup&, int)::{lambda(hipsycl::sycl::handler&)#1}::operator()(hipsycl::sycl::handler&) const::{lambda(hipsycl::sycl::nd_item<(int)2>)#1}>(void, (int)2, hipsycl::sycl::range<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)>, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>), hipsycl::sycl::id<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)>, unsigned long, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>))::{lambda(void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)&)#1}::operator()<>(, signed char, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)) const::{lambda(hipsycl::sycl::id<(int)2>hipsycl::sycl::id<(int)2>)#2}>::_M_invoke  Electrostatic2D3V   12.551s              3.6%
[Others]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              N/A                194.241s             56.0%
Collection and Platform Info
    Application Command Line: /cosma5/data/do009/dc-saun1/git/NESO3D/solvers/Electrostatic2D3V/Electrostatic2D3V "conditions.xml" "square_32x32.xml" 
    Operating System: 3.10.0-1160.83.1.el7.x86_64 \S Kernel \r on an \m 
    MPI Process Rank: 44
    Computer Name: b105.pri.cosma7.alces.network
    Result Size: 22.9 MB 
    Collection start time: 15:59:54 04/05/2023 UTC
    Collection stop time: 16:00:26 04/05/2023 UTC
    Collector Type: User-mode sampling and tracing
    CPU
        Name: Unknown
        Frequency: 2.994 GHz
        Logical CPU Count: 64
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
Elapsed Time: 13.664s
    CPU Time: 339.419s
        Effective Time: 237.739s
        Spin Time: 101.551s
         | A significant portion of CPU time is spent waiting. Use this metric
         | to discover which synchronizations are spinning. Consider adjusting
         | spin wait parameters, changing the lock implementation (for example,
         | by backing off then descheduling), or adjusting the synchronization
         | granularity.
         |
            Imbalance or Serial Spinning: 97.255s
             | The threading runtime function related to time spent on imbalance
             | or serial spinning consumed a significant amount of CPU time.
             | This can be caused by a load imbalance, insufficient concurrency
             | for all working threads, or busy waits of worker threads while
             | serial code is executed. If there is an imbalance, apply dynamic
             | work scheduling or reduce the size of work chunks or tasks. If
             | there is insufficient concurrency, consider collapsing the outer
             | and inner loops. If there is a wait for completion of serial
             | code, explore options for parallelization with Intel Advisor,
             | algorithm, or microarchitecture tuning of the application's
             | serial code with VTune Profiler Basic Hotspots or
             | Microarchitecture Exploration analysis respectively. For OpenMP*
             | applications, use the Per-Barrier OpenMP Potential Gain metric
             | set in the HPC Performance Characterization analysis to discover
             | the reason for high imbalance or serial spin time.
             |
            Lock Contention: 0s
            Other: 4.296s
        Overhead Time: 0.130s
            Creation: 0.130s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 144
    Paused Time: 0s

Top Hotspots
Function                                                                                                                                                                                                                                                                                                                                                                            Module             CPU Time  % of CPU Time(%)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------  --------  ----------------
gomp_simple_barrier_wait                                                                                                                                                                                                                                                                                                                                                            libgomp.so.1        84.920s             25.0%
MPI_Wait                                                                                                                                                                                                                                                                                                                                                                            libmpi.so.40        24.831s              7.3%
MPI_Bcast                                                                                                                                                                                                                                                                                                                                                                           libmpi.so.40        23.820s              7.0%
_ZN7hipsycl4glue12omp_dispatch29reducible_parallel_invocationIZNS1_19parallel_for_kernelILi1EZZN4NESO16BaryEvaluateBaseIN6Nektar12MultiRegions9ContFieldEE8evaluateIdNS6_5ArrayINS6_4OneDEdEEEEvSt10shared_ptrINS4_9Particles13ParticleGroupEENSF_3SymIT_EEiRKT0_ENKUlRNS_4sycl7handlerEE_clESQ_EUlNSO_2idILi1EEEE_JEEEvSL_NSO_5rangeIXT_EEEDpT1_EUlDpRT_E_JEEEvSJ_DpT0_._omp_fn.0  Electrostatic2D3V   14.362s              4.2%
uct_mm_iface_progress                                                                                                                                                                                                                                                                                                                                                               libuct.so.0         13.560s              4.0%
[Others]                                                                                                                                                                                                                                                                                                                                                                            N/A                177.926s             52.4%
Collection and Platform Info
    Application Command Line: /cosma5/data/do009/dc-saun1/git/NESO3D/solvers/Electrostatic2D3V/Electrostatic2D3V "conditions.xml" "square_32x32.xml" 
    Operating System: 3.10.0-1160.83.1.el7.x86_64 \S Kernel \r on an \m 
    MPI Process Rank: 78
    Computer Name: b107.pri.cosma7.alces.network
    Result Size: 22.8 MB 
    Collection start time: 16:00:02 04/05/2023 UTC
    Collection stop time: 16:00:26 04/05/2023 UTC
    Collector Type: User-mode sampling and tracing
    CPU
        Name: Unknown
        Frequency: 2.994 GHz
        Logical CPU Count: 64
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
Elapsed Time: 21.522s
    CPU Time: 345.628s
        Effective Time: 245.437s
        Spin Time: 100.121s
         | A significant portion of CPU time is spent waiting. Use this metric
         | to discover which synchronizations are spinning. Consider adjusting
         | spin wait parameters, changing the lock implementation (for example,
         | by backing off then descheduling), or adjusting the synchronization
         | granularity.
         |
            Imbalance or Serial Spinning: 96.082s
             | The threading runtime function related to time spent on imbalance
             | or serial spinning consumed a significant amount of CPU time.
             | This can be caused by a load imbalance, insufficient concurrency
             | for all working threads, or busy waits of worker threads while
             | serial code is executed. If there is an imbalance, apply dynamic
             | work scheduling or reduce the size of work chunks or tasks. If
             | there is insufficient concurrency, consider collapsing the outer
             | and inner loops. If there is a wait for completion of serial
             | code, explore options for parallelization with Intel Advisor,
             | algorithm, or microarchitecture tuning of the application's
             | serial code with VTune Profiler Basic Hotspots or
             | Microarchitecture Exploration analysis respectively. For OpenMP*
             | applications, use the Per-Barrier OpenMP Potential Gain metric
             | set in the HPC Performance Characterization analysis to discover
             | the reason for high imbalance or serial spin time.
             |
            Lock Contention: 0s
            Other: 4.039s
        Overhead Time: 0.070s
            Creation: 0.070s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 144
    Paused Time: 0s

Top Hotspots
Function                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Module             CPU Time  % of CPU Time(%)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------  --------  ----------------
gomp_simple_barrier_wait                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              libgomp.so.1        82.683s             23.9%
MPI_Bcast                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             libmpi.so.40        22.598s              6.5%
MPI_Wait                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              libmpi.so.40        21.036s              6.1%
_ZN7hipsycl4glue12omp_dispatch29reducible_parallel_invocationIZNS1_19parallel_for_kernelILi1EZZN4NESO16BaryEvaluateBaseIN6Nektar12MultiRegions9ContFieldEE8evaluateIdNS6_5ArrayINS6_4OneDEdEEEEvSt10shared_ptrINS4_9Particles13ParticleGroupEENSF_3SymIT_EEiRKT0_ENKUlRNS_4sycl7handlerEE_clESQ_EUlNSO_2idILi1EEEE_JEEEvSL_NSO_5rangeIXT_EEEDpT1_EUlDpRT_E_JEEEvSJ_DpT0_._omp_fn.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Electrostatic2D3V   15.218s              4.4%
std::_Function_handler<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>), hipsycl::glue::omp_dispatch::parallel_for_ndrange_kernel<(int)2, NESO::NektarGraphLocalMapperT::map_native_2d(NESO::Particles::ParticleGroup&, int)::{lambda(hipsycl::sycl::handler&)#1}::operator()(hipsycl::sycl::handler&) const::{lambda(hipsycl::sycl::nd_item<(int)2>)#1}>(void, (int)2, hipsycl::sycl::range<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)>, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>), hipsycl::sycl::id<void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)>, unsigned long, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>))::{lambda(void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)&)#1}::operator()<>(, signed char, void (hipsycl::sycl::id<(int)2>, hipsycl::sycl::id<(int)2>)) const::{lambda(hipsycl::sycl::id<(int)2>hipsycl::sycl::id<(int)2>)#2}>::_M_invoke  Electrostatic2D3V   12.533s              3.6%
[Others]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              N/A                191.562s             55.4%
Collection and Platform Info
    Application Command Line: /cosma5/data/do009/dc-saun1/git/NESO3D/solvers/Electrostatic2D3V/Electrostatic2D3V "conditions.xml" "square_32x32.xml" 
    Operating System: 3.10.0-1160.83.1.el7.x86_64 \S Kernel \r on an \m 
    MPI Process Rank: 114
    Computer Name: b110.pri.cosma7.alces.network
    Result Size: 23.4 MB 
    Collection start time: 15:59:54 04/05/2023 UTC
    Collection stop time: 16:00:26 04/05/2023 UTC
    Collector Type: User-mode sampling and tracing
    CPU
        Name: Unknown
        Frequency: 2.994 GHz
        Logical CPU Count: 64
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
Elapsed Time: 19.843s
    CPU Time: 342.039s
        Effective Time: 241.606s
        Spin Time: 100.334s
         | A significant portion of CPU time is spent waiting. Use this metric
         | to discover which synchronizations are spinning. Consider adjusting
         | spin wait parameters, changing the lock implementation (for example,
         | by backing off then descheduling), or adjusting the synchronization
         | granularity.
         |
            Imbalance or Serial Spinning: 95.955s
             | The threading runtime function related to time spent on imbalance
             | or serial spinning consumed a significant amount of CPU time.
             | This can be caused by a load imbalance, insufficient concurrency
             | for all working threads, or busy waits of worker threads while
             | serial code is executed. If there is an imbalance, apply dynamic
             | work scheduling or reduce the size of work chunks or tasks. If
             | there is insufficient concurrency, consider collapsing the outer
             | and inner loops. If there is a wait for completion of serial
             | code, explore options for parallelization with Intel Advisor,
             | algorithm, or microarchitecture tuning of the application's
             | serial code with VTune Profiler Basic Hotspots or
             | Microarchitecture Exploration analysis respectively. For OpenMP*
             | applications, use the Per-Barrier OpenMP Potential Gain metric
             | set in the HPC Performance Characterization analysis to discover
             | the reason for high imbalance or serial spin time.
             |
            Lock Contention: 0s
            Other: 4.379s
        Overhead Time: 0.099s
            Creation: 0.099s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 144
    Paused Time: 0s

Top Hotspots
Function                                                                                                                                                                                                                                                                                                                                                                            Module             CPU Time  % of CPU Time(%)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -----------------  --------  ----------------
gomp_simple_barrier_wait                                                                                                                                                                                                                                                                                                                                                            libgomp.so.1        84.464s             24.7%
MPI_Wait                                                                                                                                                                                                                                                                                                                                                                            libmpi.so.40        24.379s              7.1%
MPI_Bcast                                                                                                                                                                                                                                                                                                                                                                           libmpi.so.40        23.381s              6.8%
_ZN7hipsycl4glue12omp_dispatch29reducible_parallel_invocationIZNS1_19parallel_for_kernelILi1EZZN4NESO16BaryEvaluateBaseIN6Nektar12MultiRegions9ContFieldEE8evaluateIdNS6_5ArrayINS6_4OneDEdEEEEvSt10shared_ptrINS4_9Particles13ParticleGroupEENSF_3SymIT_EEiRKT0_ENKUlRNS_4sycl7handlerEE_clESQ_EUlNSO_2idILi1EEEE_JEEEvSL_NSO_5rangeIXT_EEEDpT1_EUlDpRT_E_JEEEvSJ_DpT0_._omp_fn.0  Electrostatic2D3V   15.128s              4.4%
uct_mm_iface_progress                                                                                                                                                                                                                                                                                                                                                               libuct.so.0         14.146s              4.1%
[Others]                                                                                                                                                                                                                                                                                                                                                                            N/A                180.542s             52.8%
Collection and Platform Info
    Application Command Line: /cosma5/data/do009/dc-saun1/git/NESO3D/solvers/Electrostatic2D3V/Electrostatic2D3V "conditions.xml" "square_32x32.xml" 
    Operating System: 3.10.0-1160.83.1.el7.x86_64 \S Kernel \r on an \m 
    MPI Process Rank: 91
    Computer Name: b108.pri.cosma7.alces.network
    Result Size: 25.0 MB 
    Collection start time: 15:59:55 04/05/2023 UTC
    Collection stop time: 16:00:26 04/05/2023 UTC
    Collector Type: User-mode sampling and tracing
    CPU
        Name: Unknown
        Frequency: 2.994 GHz
        Logical CPU Count: 64
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
